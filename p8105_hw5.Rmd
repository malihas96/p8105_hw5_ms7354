---
title: "p8105_hw5"
author: "Maliha Safdar"
date: "2025-11-06"
output: github_document
---


```{r}
library(tidyverse)
library(broom)
```


## Problem 1


`In this question we will construct a function for birthdays which will produce a result such that when n number of people are put into a room, at least two people have repeating birthdays. We will know that the result is acheived when we run the bday_sim (birthday simulation) and the result is true.`

```{r}

bday_sim = function(n_room) {
  
  birthdays = sample( 1:365, n_room, replace = TRUE) #n_room the no. of people in a room
  
  repeated_bday = length(unique(birthdays)) < n_room # this means that if the length of unique birthdays is less than people in the room, than at least two people have repeated birthdays
  
  repeated_bday
}

bday_sim(50) #running the simulation 20 times. The result was false which means no repeated birthdays yet. When it was repeated 50 times, the result was true because the chance of increases when we increase the number of times people are put into the room and they share sam birthdays. 

```

`Next, the function will be run 10000 for groups 2 to 50 (which is number of people that will be put in the room) then, a probability plot will be constructed to show how the probabilty changes as we increase the group size when running the function 10000 times.`

```{r}
bday_sim_results = 
  expand.grid(
    bdays = 2:50,
    iter = 1:50
  ) |>
  mutate(
    result = map_lgl(bdays, bday_sim)
  ) |>
    group_by(
      bdays
    ) |>
  summarize(
    prob_repeat = mean(result)
  )


bday_sim_results |>
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line()

```

`The probability curve above begins near 0, when n =2 and rises slowly and then increases steeply. At n = 22-23, the probability of having atleast two same birthdays reaches 50%. Therefore, we can conclude that when the number of people that are put in a room increases, the probablity that we have repeated birthdays (of at least two people) increases as well. We should also note that the number of times the function is run also plays a key role in this increase, since we ran it 10000 times.`


### Problem 2

`We will first set up the design elements to form a normal distribution which has a  sample size 30, mean = 0 and sd = 3. And then generate 5000 datasets of sample size 30.`

```{r}
sim_mean_sd = function( n = 30, mu = 0, sigma = 5) {
  sim_data = tibble (
   x = rnorm(n,mean = mu, sd = sigma)
  )
  
  sim_data |>
    summarize(
      mu_hat = mean(x),
      sigma_hat = sd(x)
    )
}

sim_results_df = expand_grid(
  sample_size = 30,
  iter = 1:5000
) |>
  mutate(
    estimate_df = map( iter, ~sim_mean_sd ( n = sample_size))
    ) |>
  unnest(estimate_df)

```

`Now we will construct a function which will be the power for the sample t.test (proportion of times the null hypothesis is rejcted).`

```{r}

sim_hypo = function( n = 30, mu = 0, sigma = 5) {
  
  x = rnorm(n, mean = mu, sd = sigma)
  
  tidy(t.test(x, mu = 0)) |>
    select(p.value) |>
    mutate(mu_hat = mean(x))
  
}

mus = 1:6  

power_results = map_dfr(mus, function(true_mu) {
  
  sims = map_dfr(1:5000, ~ sim_hypo(mu = true_mu))
  
  tibble(
    mu = true_mu,
    power = mean(sims$p.value < 0.05)
  )
  
})

power_results |>
  ggplot(aes(x = mu, y = power)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Power Curve for One Sample t-test",
    x = "True Mean (μ)",
    y = "Power"
  )
```
`We observe that as the true mean size increases, the power of the t.test also increases (which is the proportion of times the null hypothesis : μ = 0 was rejected).Therefore, the larger the effect size, the higher the power.`


`Next we will construct a plot which will show the average estimate of mu_hat and true value of mu.`

```{r}

sim_true_mu = function( n = 30, mu = 0, sigma = 5) {
  
  x = rnorm(n, mean = mu, sd = sigma)
  tibble(
    mu_hat = mean(x),
    p_value = tidy(t.test(x, mu = 0))$p.value
  )

}


sim_true_mu_results = expand_grid(
  true_mu = 1:6,
  iter = 1:5000
) |>
  mutate(
    sim = map( true_mu, ~sim_true_mu ( mu = .x))
    ) |>
  unnest(sim)

#finding average estimate of mu hat.

avg_mu_estimate = sim_true_mu_results |>
  group_by(true_mu) |>
  summarise(
    avg_mu_hat = mean(mu_hat)
  )

avg_mu_estimate |>
  ggplot(aes(x = true_mu, y = avg_mu_hat)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Average Estimate of mu_hat vs True mu",
    x = "true mu",
    y = "average mu"
  )

```
`Now we will construct a second plot which will display the average estimate mu_hat only for the samples that had the null hypothesis reject.`

```{r}
reject_mu_hat = sim_true_mu_results |>
  filter(p_value < 0.05) |>
  group_by(true_mu) |>
  summarise(
    avg_mu_hat_reject = mean(mu_hat)
  )

reject_mu_hat |>
  ggplot(aes(x = true_mu, y = avg_mu_hat_reject)) +
  geom_point(color = "red") +
  geom_line() +
  labs(
    title = "Average Estimate of mu_hat  (in which null is rejected) vs True mu",
    x = "true mu",
    y = "average est. of mu_hat (null rejected)"
  )
```

`Comparing both plots, we can see that the average mu_hat which was rejected for null hypothesis does not equal true mu. The difference is more noticeable for smaller values of true mu, and as it increases (to true mu = 6) the difference on the plot is not very apparent. Earlier we established that larger size effect will increase power. When the power is low then the mu_hat is larger than the true mu, whereas when power is high the true mu and mu_hat approach equality.`

## Problem 3

```{r}
homicide_data = read.csv("homicide-data.csv")
```

`The raw dataset includes data on more than 52,000 criminal homicides over the past decade in 50 of the largest American cities. It includes variables such as the victims' first and last names, their age, gender, the date the homicide was report, the location of the homicide (city, state and map cordinates), and whether their case was closed with no arrest/arrest made or if it is still open.`

```{r}
unsolved_homicide_data <- homicide_data |>
  mutate(
    city_state = str_c(city, state, sep = ","),
    unsolved = disposition%in%c("Closed without arrest", "Open/No arrest")
  ) |>
  group_by(city_state) |>
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(unsolved, na.rm = TRUE))

knitr::kable(head(unsolved_homicide_data, 5))
  
```

`Now we will first find the proportion of unsolved homicides and yje confidence intervals for Baltimore,MD and then for all the cities that are contained within the dataset using prop.test and broom::tidy.`

```{r}
baltimore_unsolved_counts <- homicide_data |>
    mutate(
      city_state = str_c(city, ",", state), 
      unsolved = disposition%in%c("Closed without arrest", "Open/No arrest")
    ) |>
    filter(city_state == "Baltimore,MD") |>
    summarise(
      total_homicides = n(),
      unsolved_homicides = sum(unsolved, na.rm = TRUE),
      .groups = "drop"
    )


baltimore_prop_test <- prop.test(
  baltimore_unsolved_counts$unsolved_homicides,
  baltimore_unsolved_counts$total_homicides,
  correct = FALSE
)

baltimore_tidy <- tidy(baltimore_prop_test) #this shows us our estimate proportion and confidence intervals but we need to pull it.

baltimore_result <- baltimore_tidy |>
  select(
    unsolved_homicides_prop = estimate,
    ci_low = conf.low,
    ci_high = conf.high
  )

baltimore_result
```

```{r}

all_cities_prop_ci <- homicide_data |>
  mutate(
    city_state = str_c(city, ",", state), 
      unsolved = disposition%in%c("Closed without arrest", "Open/No arrest")
  
    ) |>
  group_by(city_state) |>
  summarise(
      total_homicides = n(),
      unsolved_homicides = sum(unsolved, na.rm = TRUE),
      .groups = "drop"
  ) |>
  mutate(
    all_cities_result = map2(unsolved_homicides,total_homicides, ~prop.test(.x, .y, correct = FALSE)),
    all_cities_tidy_result = map(all_cities_result, tidy)
  ) |>
  unnest(all_cities_tidy_result) |>
  select(
    city_state,
    unsolved_homicides,
    total_homicides,
    unsolved_homicides_prop = estimate,
    ci_low = conf.low,
    ci_high = conf.high
  )

knitr::kable(head(all_cities_prop_ci, 6))
```

` Now we will create a plot that shows the estimates and CIs for each city.`

```{r}
all_cities_plot <- all_cities_prop_ci |>
  arrange(unsolved_homicides_prop) |>
  mutate(city_state = factor(city_state, levels = city_state))

all_cities_plot |>
  ggplot(aes(x = city_state, y = unsolved_homicides_prop)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_low, ymax = ci_high)) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 10)) +
  labs(
    title = "Proportion of Unsolved Homicide Cases in Different US Cities",
    x = "City, State",
    y = "Unsolved Homicide Proportion"
  )
  
  
```





